{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Assignment\n"
      ],
      "metadata": {
        "id": "-ZXoxyw6v-XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "**Answer:**\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression.\n",
        "In classification, it splits the dataset into smaller subsets based on feature values, forming a tree-like structure.\n",
        "Each internal node represents a decision based on a feature, each branch represents an outcome, and each leaf node represents a class label.\n",
        "The tree selects the best split using impurity measures like Gini Impurity or Entropy and continues splitting until a stopping condition is met.\n"
      ],
      "metadata": {
        "id": "JIj-_usiwD5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Answer:**\n",
        "Gini Impurity measures the probability of incorrect classification of a randomly chosen element.\n",
        "Lower Gini values indicate purer nodes.\n",
        "\n",
        "Entropy measures the level of uncertainty or randomness in the dataset.\n",
        "Lower entropy means higher purity.\n",
        "\n",
        "Decision Trees choose splits that minimize Gini Impurity or Entropy, resulting in more homogeneous child nodes.\n"
      ],
      "metadata": {
        "id": "3fC8G-IXwHAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "**Answer:**\n",
        "Pre-Pruning stops the tree growth early by setting constraints like maximum depth.\n",
        "Advantage: Reduces overfitting and computation time.\n",
        "\n",
        "Post-Pruning allows the tree to grow fully and then removes unnecessary branches.\n",
        "Advantage: Improves generalization and model accuracy.\n"
      ],
      "metadata": {
        "id": "rrgPx1RbwKas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "**Answer:**\n",
        "Information Gain measures the reduction in entropy after splitting the dataset on a feature.\n",
        "It helps identify the feature that provides the most information about the target variable.\n",
        "The feature with the highest Information Gain is selected for the split, improving classification accuracy.\n"
      ],
      "metadata": {
        "id": "mtof7FvUwN0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Answer:**\n",
        "Applications include medical diagnosis, fraud detection, credit risk analysis, and customer churn prediction.\n",
        "\n",
        "Advantages:\n",
        "- Easy to understand and interpret\n",
        "- Handles both numerical and categorical data\n",
        "\n",
        "Limitations:\n",
        "- Prone to overfitting\n",
        "- Sensitive to small changes in data\n"
      ],
      "metadata": {
        "id": "tIRVLEmXwT2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Decision Tree Classifier using Gini Criterion"
      ],
      "metadata": {
        "id": "UKsxPIy7wfhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, fetch_openml\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/mnt/data', exist_ok=True)\n",
        "\n",
        "# ----- Iris Dataset -----\n",
        "iris = load_iris(as_frame=True)\n",
        "iris_df = iris.frame\n",
        "iris_csv_path = \"/mnt/data/iris_dataset.csv\"\n",
        "iris_df.to_csv(iris_csv_path, index=False)\n",
        "\n",
        "# ----- Boston Housing Dataset (via OpenML) -----\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "boston_df = boston.frame\n",
        "boston_csv_path = \"/mnt/data/boston_housing_dataset.csv\"\n",
        "boston_df.to_csv(boston_csv_path, index=False)\n",
        "\n",
        "iris_csv_path, boston_csv_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gtaeQxhz1i1",
        "outputId": "7e621d31-0d04-4775-9028-ea69c2d80f43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/mnt/data/iris_dataset.csv', '/mnt/data/boston_housing_dataset.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBktoJIEwZkh",
        "outputId": "f9a959a4-ee56-4eb4-a315-62ef54ac6419"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: max_depth=3 vs Fully-Grown Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_acc = accuracy_score(y_test, limited_tree.predict(X_test))\n",
        "\n",
        "print(\"Fully-Grown Tree Accuracy:\", full_acc)\n",
        "print(\"Max Depth=3 Tree Accuracy:\", limited_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOfG-VeLwcL5",
        "outputId": "e7e15c3c-e262-4acb-f409-d09f3d608c5e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 1.0\n",
            "Max Depth=3 Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Decision Tree Regressor on Boston Housing Dataset\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use the boston_df already loaded from fetch_openml\n",
        "X = boston_df.drop('MEDV', axis=1)  # Features are all columns except 'MEDV'\n",
        "y = boston_df['MEDV']              # Target is the 'MEDV' column\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8VwxRmKxzta",
        "outputId": "8c38aa33-bf2a-43d6-e131-c8f65ad53402"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.416078431372549\n",
            "Feature Importances: [5.12956739e-02 3.35270585e-03 5.81619171e-03 2.27940651e-06\n",
            " 2.71483790e-02 6.00326256e-01 1.36170630e-02 7.06881622e-02\n",
            " 1.94062297e-03 1.24638653e-02 1.10116089e-02 9.00872742e-03\n",
            " 1.93328464e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor # Changed from DecisionTreeClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score # Changed metric for regression\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [None, 2, 3, 4, 5],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeRegressor(random_state=42), # Changed to DecisionTreeRegressor\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error' # Specify a scoring metric appropriate for regression\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R-squared:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqTyOGWSx3W9",
        "outputId": "fbcaca38-4729-4a2a-b984-a2eccbaf08d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'min_samples_split': 2}\n",
            "Mean Squared Error: 25.993190895971196\n",
            "R-squared: 0.6455495710736121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Healthcare Decision Tree Use Case\n",
        "\n",
        "**Answer:**\n",
        "Missing values are handled using mean or mode imputation.\n",
        "Categorical features are encoded using one-hot or label encoding.\n",
        "A Decision Tree model is trained using cleaned data.\n",
        "Hyperparameters are tuned using GridSearchCV.\n",
        "Performance is evaluated using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Business Value:\n",
        "The model helps in early disease detection, reduces healthcare costs, improves patient outcomes, and provides transparent decision-making support to doctors.\n"
      ],
      "metadata": {
        "id": "Z7twXTNpxEZr"
      }
    }
  ]
}